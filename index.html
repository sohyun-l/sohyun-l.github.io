<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Sohyun  Lee</title>
    <meta name="author" content="Sohyun  Lee" />
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
" />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/smile-o"/>
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://sohyun-l.github.io/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://sohyun-l.github.io/"><span class="font-weight-bold">Sohyun</span>   Lee</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a>
              </li>
              <!---->
              <!-- Blog -->
              <!--<li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li>-->
                
              <!-- CV -->
              <li class="nav-item">
                <a class="nav-link" href="/assets/Resume.pdf" target="_blank" rel="noopener noreferrer">Curriculum Vitae</a>
              </li>
                
              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           <span class="font-weight-bold">Sohyun</span>  Lee
          </h1>
          <p class="desc"></p>
        </header>

        <article>
          <div class="profile float-right">

              <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/SohyunLee-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/SohyunLee-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/SohyunLee-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid z-depth-1 rounded-circle" src="/assets/img/SohyunLee.png" alt="SohyunLee.png">

  </picture>

</figure>

          </div>

          <div class="clearfix">
            <p>I am a graduate student in the Graduate School of Artificial Intelligence integrated M.S. &amp; Ph.D. program at <a href="https://www.postech.ac.kr" target="_blank" rel="noopener noreferrer">POSTECH</a>. I am a member of the <a href="http://cvlab.postech.ac.kr/lab/" target="_blank" rel="noopener noreferrer">Computer Vision Lab</a> at POSTECH, advised by Prof. <a href="https://suhakwak.github.io" target="_blank" rel="noopener noreferrer">Suha Kwak</a>. 
Previously, I completed my B.S. in Mechanical Engineering at POSTECH.<br>
<br>
My research interests lie in computer vision and deep learning. I‚Äôve worked on the robust recognition in adverse visual conditions, domain adaptation, and generalization.</p>

<p>If you are interested in my research projects, please feel free to contact me.</p>

<!-- Write your biography here. Tell the world about yourself. Link to your favorite [subreddit](http://reddit.com). You can put a picture in, too. The code is already in, just name your picture `prof_pic.jpg` and put it in the `img/` folder.

Put your address / P.O. box / other info right below your picture. You can also disable any these elements by editing `profile` property of the YAML header of your `_pages/about.md`. Edit `_bibliography/papers.bib` and Jekyll will render your [publications page](/al-folio/publications/) automatically.


Link to your social media connections, too. This theme is set up to use [Font Awesome icons](http://fortawesome.github.io/Font-Awesome/) and [Academicons](https://jpswalsh.github.io/academicons/), like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them. -->

          <!-- Social -->
          <div class="social">
            <div class="contact-icons">
            <a href="mailto:%6C%73%68%69%67%39%36@%70%6F%73%74%65%63%68.%61%63.%6B%72" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://scholar.google.com/citations?user=S_IGo4UAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/sohyun-l" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/sohyun-lee-858616233" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>
            
            </div>
                
            <div class="contact-note">
              
            </div>
            
          </div>
          </div>

          <!-- News -->          
          <div class="news">
            <h2>News</h2>
            <div class="table-responsive">
              <table class="table table-sm table-borderless"> 
                <tr>
                  <th scope="row">Sep 19, 2025</th>
                  <td>
                    üìù Our paper on improving robustness of Segment Anything Model has been accepted to <a href="https://neurips.cc/" target="_blank" rel="noopener noreferrer">NeurIPS 2025</a>.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Jul 2, 2024</th>
                  <td>
                    üìù Our paper on robust segmentation under multiple adverse conditions is accepted to <a href="https://eccv.ecva.net" target="_blank" rel="noopener noreferrer">ECCV 2024</a>.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Sep 22, 2023</th>
                  <td>
                    üìù Our paper on active learning is accepted to <a href="https://nips.cc" target="_blank" rel="noopener noreferrer">NeurIPS 2023</a>.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Jun 10, 2023</th>
                  <td>
                    <img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> I won the POSTECHIAN Fellowship.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Feb 28, 2023</th>
                  <td>
                    üìù Our paper on low-light image recognition is accepted to <a href="https://cvpr2023.thecvf.com" target="_blank" rel="noopener noreferrer">CVPR 2023</a>.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Feb 27, 2023</th>
                  <td>
                    <img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> FIFO won the grand prize at BK21 Best Paper Award from <a href="https://ai.postech.ac.kr" target="_blank" rel="noopener noreferrer">POSTECH GSAI</a>.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Feb 8, 2023</th>
                  <td>
                    <img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> I won the excellence award at 3rd POSTECH Research Performance Contest.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Nov 7, 2022</th>
                  <td>
                    <img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> Three papers got honored to be the winners at <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2022-south-korea" target="_blank" rel="noopener noreferrer">the Qualcomm Innovation Fellowship 2022</a>.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Jul 4, 2022</th>
                  <td>
                    üìù A paper on active domain adaptation is accepted to <a href="https://eccv2022.ecva.net/" target="_blank" rel="noopener noreferrer">ECCV 2022</a>.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Jun 21, 2022</th>
                  <td>
                    <img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> Our paper on foggy scene segmentation is nominated as a best paper finalist in <a href="https://cvpr2022.thecvf.com" target="_blank" rel="noopener noreferrer">CVPR 2022</a>.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Mar 3, 2022</th>
                  <td>
                    üìù Two papers (including one best paper finalist) are accepted to <a href="https://cvpr2022.thecvf.com" target="_blank" rel="noopener noreferrer">CVPR 2022</a>.
 
                  </td>
                </tr> 
              </table>
            </div> 
          </div>


          <!-- Experience -->          <div class="experience">
            <h2>Experience</h2>
            <div class="table-responsive">
              <table class="table table-sm table-borderless"> 
                <tr>
                  <th scope="row">May, 2025 - Aug., 2025</th>
                  <td>
                    <a href="https://ethz.ch/en.html" target="_blank" rel="noopener noreferrer">ETH Z√ºrich</a>, Z√ºrich, Switzerland <br>
<em>Visiting Research Student</em>
<ul>
  <li>Host: Dr. <a href="https://people.ee.ethz.ch/~csakarid/" target="_blank" rel="noopener noreferrer">Christos Sakaridis</a>, Prof. <a href="https://prs.igp.ethz.ch/group/people/person-detail.schindler.html" target="_blank" rel="noopener noreferrer">Konrad Schindler</a>
</li>
</ul>
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Jan, 2025 - Present</th>
                  <td>
                    Google Z√ºrich <br>
<em>Research Collaboration</em>
<ul>
  <li>Working with <a href="https://lhoyer.github.io/" target="_blank" rel="noopener noreferrer">Dr. Lukas Hoyer</a>
</li>
</ul>
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Mar, 2024 - May, 2024</th>
                  <td>
                    <a href="https://tuebingen.ai/" target="_blank" rel="noopener noreferrer">T√ºbingen AI Center, University of T√ºbingen</a>, T√ºbingen, Germany <br>
<em>Visiting Research Student</em>
<ul>
  <li>Host: Prof. <a href="https://coallaoh.github.io/" target="_blank" rel="noopener noreferrer">Seong Joon Oh</a>
</li>
</ul>
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Sep, 2020 - Present</th>
                  <td>
                    <a href="http://cvlab.postech.ac.kr/lab/" target="_blank" rel="noopener noreferrer">Computer Vision Lab</a>, <a href="https://postech.ac.kr/" target="_blank" rel="noopener noreferrer">POSTECH</a>, Pohang, South Korea <br>
<em>Research and Teaching Assistant</em>
 
                  </td>
                </tr> 
              </table>
            </div> 
          </div>

              
          <!-- Education -->          <div class="education">
            <h2>Education</h2>
            <div class="table-responsive">
              <table class="table table-sm table-borderless"> 
                <tr>
                  <th scope="row">Sep, 2020 - Present</th>
                  <td>
                    <a href="https://www.postech.ac.kr" target="_blank" rel="noopener noreferrer">Pohang University of Science and Technology (POSTECH)</a>, Pohang, South Korea <br>
Integrated M.S. &amp; Ph.D. Student in Artificial Intelligence <br>
Advisor: Prof. <a href="https://suhakwak.github.io/index.html" target="_blank" rel="noopener noreferrer">Suha Kwak</a>.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Mar, 2015 - Aug, 2020</th>
                  <td>
                    <a href="https://www.postech.ac.kr" target="_blank" rel="noopener noreferrer">Pohang University of Science and Technology (POSTECH)</a>, Pohang, South Korea <br>
B.S in Mechanical Engineering <br>
Advisor: Prof. <a href="https://scholar.google.com/citations?user=jdNQRH8AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Junsuk Rho</a>.
 
                  </td>
                </tr> 
              </table>
            </div> 
          </div>

              
          <!-- Selected papers -->
          <div class="publications">
            <h2>Publications</h2>
            <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">            
      <div class="col-sm-3">
        
      </div>
            
        <!-- Entry bib key -->
        <div id="lee2026moga" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Robust Promptable Video Object Segmentation</div>
          <!-- Author -->
          <div class="author">
                  <em>Sohyun Lee</em>,¬†Yeho Gwon,¬†Lukas Hoyer,¬†Konrad Schindler,¬†Christos Sakaridis,¬†and Suha Kwak  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Under Review</em> 2026
          </div>
              
            <!---->
           
            
              
           <!-- Additional info -->
            

           <!-- Additional info3 -->
            
    
           <!-- Additional info2 -->
            
            
              
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The performance of promptable video object segmentation (PVOS) models substantially degrades under input corruptions, which prevents PVOS deployment in safety-critical domains. This paper offers the first comprehensive study on robust PVOS (RobustPVOS). We first construct a new, comprehensive benchmark with two real-world evaluation datasets of 351 video clips and more than 2,500 object masks under real-world adverse conditions. At the same time, we generate synthetic training data by applying diverse and temporally varying corruptions to existing VOS datasets. Moreover, we present a new RobustPVOS method, dubbed Memory-object-conditioned Gated-rank Adaptation (MoGA). The key to successfully performing RobustPVOS is two-fold: effectively handling object-specific degradation and ensuring temporal consistency in predictions. MoGA leverages object-specific representations maintained in memory across frames to condition the robustification process, which allows the model to handle each tracked object differently in a temporally consistent way. Extensive experiments on our benchmark validate MoGA‚Äôs efficacy, showing consistent and significant improvements across diverse corruption types on both synthetic and real-world datasets, establishing a strong baseline for future RobustPVOS research. Our benchmark will be made publicly available.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">            
      <div class="col-sm-3">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/garasam_teaser-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/garasam_teaser-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/garasam_teaser-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/garasam_teaser.png" data-zoomable>

  </picture>

</figure>

      </div>
            
        <!-- Entry bib key -->
        <div id="lee2025garasam" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">GaRA-SAM: Robustifying Segment Anything Model with Gated-Rank Adaptation</div>
          <!-- Author -->
          <div class="author">
                  <em>Sohyun Lee</em>,¬†Yeho Gwon,¬†Lukas Hoyer,¬†and Suha Kwak  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Conference on Neural Information Processing Systems (<b>NeurIPS</b>),</em> 2025
          </div>
              
            <!---->
           
            
              
           <!-- Additional info -->
            

           <!-- Additional info3 -->
            
    
           <!-- Additional info2 -->
            <span style="color:green">ICCV 2025 Workshop on Building Foundation Models You Can Trust</span>
            
              
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2506.02882" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Improving robustness of the Segment Anything Model (SAM) to input degradations is critical for its deployment in high-stakes applications such as autonomous driving and robotics. Our approach to this challenge prioritizes three key aspects: first, parameter efficiency to maintain the inherent generalization capability of SAM; second, fine-grained and input-aware robustification to precisely address the input corruption; and third, adherence to standard training protocols for ease of training. To this end, we propose gated-rank adaptation (GaRA). GaRA introduces lightweight adapters into intermediate layers of the frozen SAM, where each adapter dynamically adjusts the effective rank of its weight matrix based on the input by selectively activating (rank-1) components of the matrix using a learned gating module. This adjustment enables fine-grained and input-aware robustification without compromising the generalization capability of SAM. Our model, GaRA-SAM, significantly outperforms prior work on all robust segmentation benchmarks. In particular, it surpasses the previous best IoU score by up to 21.3%p on ACDC, a challenging real corrupted image dataset.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">            
      <div class="col-sm-3">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/testdg_teaser-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/testdg_teaser-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/testdg_teaser-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/testdg_teaser.png" data-zoomable>

  </picture>

</figure>

      </div>
            
        <!-- Entry bib key -->
        <div id="lee2025dicotta" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">TestDG: Test-time Domain Generalization for Continual Test-time Adaptation</div>
          <!-- Author -->
          <div class="author">
                  <em>Sohyun Lee</em>,¬†Nayeong Kim,¬†Juwon Kang,¬†Seongjoon Oh,¬†and Suha Kwak  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>arXiv preprint,</em> 2025
          </div>
              
            <!---->
           
            
              
           <!-- Additional info -->
            

           <!-- Additional info3 -->
            
    
           <!-- Additional info2 -->
            
            
              
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2504.04981" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://github.com/sohyun-l/TestDG" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This paper studies continual test-time adaptation (CTTA), the task of adapting a model to constantly changing unseen domains in testing while preserving previously learned knowledge. Existing CTTA methods mostly focus on adaptation to the current test domain only, overlooking generalization to arbitrary test domains a model may face in the future. To tackle this limitation, we present a novel online test-time domain generalization framework for CTTA, dubbed TestDG. TestDG aims to learn features invariant to both current and previous test domains on the fly during testing, improving the potential for effective generalization to future domains. To this end, we propose a new model architecture and a test-time adaptation strategy dedicated to learning domain-invariant features, along with a new data structure and optimization algorithm for effectively managing information from previous test domains. TestDG achieved state of the art on four public CTTA benchmarks. Moreover, it showed superior generalization to unseen test domains.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">            
      <div class="col-sm-3">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/frest-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/frest-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/frest-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/frest.png" data-zoomable>

  </picture>

</figure>

      </div>
            
        <!-- Entry bib key -->
        <div id="lee2024frest" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">FREST: Feature RESToration for Semantic Segmentation under Multiple Adverse Conditions</div>
          <!-- Author -->
          <div class="author">
                  <em>Sohyun Lee</em>,¬†Namyup Kim,¬†Sungyeon Kim,¬†and Suha Kwak  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>European Conference on Computer Vision (<b>ECCV</b>),</em> 2024
          </div>
              
            <!---->
           
            
              
           <!-- Additional info -->
            

           <!-- Additional info3 -->
            
    
           <!-- Additional info2 -->
            
            
              
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2407.13437" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://sohyun-l.github.io/frest/" class="btn btn-sm z-depth-0" role="button">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Robust semantic segmentation under adverse conditions is of great importance in real-world applications. To address this challenging task in practical scenarios where labeled normal condition images are not accessible in training, we propose FREST, a novel feature restoration framework for source-free domain adaptation (SFDA) of semantic segmentation to adverse conditions. FREST alternates two steps: (1) learning the condition embedding space that only separates the condition information from the features and (2) restoring features of adverse condition images on the learned condition embedding space. By alternating these two steps, FREST gradually restores features where the effect of adverse conditions is reduced. FREST achieved a state of the art on two public benchmarks (\ie, ACDC and RobotCar) for SFDA to adverse conditions. Moreover, it shows superior generalization ability on unseen datasets.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">            
      <div class="col-sm-3">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/active_learning-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/active_learning-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/active_learning-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/active_learning.png" data-zoomable>

  </picture>

</figure>

      </div>
            
        <!-- Entry bib key -->
        <div id="sehyun2023active" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Active Learning for Semantic Segmentation with Multi-class Label Query</div>
          <!-- Author -->
          <div class="author">Sehyun Hwang,¬†
                  <em>Sohyun Lee</em>,¬†Hoyoung Kim,¬†Minhyeon Oh,¬†Jungseul Ok,¬†and Suha Kwak  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Conference on Neural Information Processing Systems (<b>NeurIPS</b>),</em> 2023
          </div>
              
            <!---->
           
            
              
           <!-- Additional info -->
            

           <!-- Additional info3 -->
            
    
           <!-- Additional info2 -->
            
            
              
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2309.09319" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://github.com/sehyun03/MulActSeg" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This paper proposes a new active learning method for semantic segmentation. The core of our method lies in a new annotation query design. It samples informative local image regions (, superpixels), and for each of such regions, asks an oracle for a multi-hot vector indicating all classes existing in the region. This multi-class labeling strategy is substantially more efficient than existing ones like segmentation, polygon, and even dominant class labeling in terms of annotation time per click. However, it introduces the class ambiguity issue in training since it assigns partial labels (, a set of candidate classes) to individual pixels. We thus propose a new algorithm for learning semantic segmentation while disambiguating the partial labels in two stages. In the first stage, it trains a segmentation model directly with the partial labels through two new loss functions motivated by partial label learning and multiple instance learning. In the second stage, it disambiguates the partial labels by generating pixel-wise pseudo labels, which are used for supervised learning of the model. Equipped with a new acquisition function dedicated to the multi-class labeling, our method outperformed previous work on Cityscapes and PASCAL VOC 2012 while spending less annotation cost.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">            
      <div class="col-sm-3">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/pid-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/pid-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/pid-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/pid.png" data-zoomable>

  </picture>

</figure>

      </div>
            
        <!-- Entry bib key -->
        <div id="lee2023pid" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Human Pose Estimation in Extremely Low-Light Conditions</div>
          <!-- Author -->
          <div class="author">
                  <em>Sohyun Lee*</em>,¬†Jaesung Rim*,¬†Boseung Jeong,¬†Geonu Kim,¬†ByungJu Woo,¬†Haechan Lee,¬†Sunghyun Cho,¬†and Suha Kwak
             (*equal contribution)  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>),</em> 2023
          </div>
              
            <!---->
           
            
              
           <!-- Additional info -->
            

           <!-- Additional info3 -->
            
    
           <!-- Additional info2 -->
            
            
              
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2303.15410" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://github.com/sohyun-l/ExLPose" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="http://cg.postech.ac.kr/research/ExLPose/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We study human pose estimation in extremely low-light images. This task is challenging due to the difficulty of collecting real low-light images with accurate labels, and severely corrupted inputs that degrade prediction quality significantly. To address the first issue, we develop a dedicated camera system and build a new dataset of real low-light images with accurate pose labels. Thanks to our camera system, each low-light image in our dataset is coupled with an aligned well-lit image, which enables accurate pose labeling and is used as privileged information during training. We also propose a new model and a new training strategy that fully exploit the privileged information to learn representation insensitive to lighting conditions. Our method demonstrates outstanding performance on real extremely low-light images, and extensive analyses validate that both of our model and dataset contribute to the success.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">            
      <div class="col-sm-3">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/sehyun-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/sehyun-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/sehyun-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/sehyun.png" data-zoomable>

  </picture>

</figure>

      </div>
            
        <!-- Entry bib key -->
        <div id="sehyun2022combating" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Combating Label Distribution Shift for Active Domain Adaptation</div>
          <!-- Author -->
          <div class="author">Sehyun Hwang,¬†
                  <em>Sohyun Lee</em>,¬†Sungyeon Kim,¬†Jungseul Ok,¬†and Suha Kwak  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>European Conference on Computer Vision (<b>ECCV</b>),</em> 2022
          </div>
              
            <!---->
           
            
              
           <!-- Additional info -->
            

           <!-- Additional info3 -->
            
    
           <!-- Additional info2 -->
            <span style="color:green">Qualcomm Innovation Fellowship Winner</span>
            
              
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2208.06604" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://github.com/sehyun03/ADA-label-distribution-matching" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="http://cvlab.postech.ac.kr/research/LAMDA/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We consider the problem of active domain adaptation (ADA) to unlabeled target data, of which subset is actively selected and labeled given a budget constraint. Inspired by recent analysis on a critical issue from label distribution mismatch between source and target in domain adaptation, we devise a method that addresses the issue for the first time in ADA. At its heart lies a novel sampling strategy, which seeks target data that best approximate the entire target distribution as well as being representative, diverse, and uncertain. The sampled target data are then used not only for supervised learning but also for matching label distributions of source and target domains, leading to remarkable performance improvement. On four public benchmarks, our method substantially outperforms existing methods in every adaptation scenario.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">            
      <div class="col-sm-3">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/fifo-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/fifo-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/fifo-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/fifo.png" data-zoomable>

  </picture>

</figure>

      </div>
            
        <!-- Entry bib key -->
        <div id="lee2022fifo" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">FIFO: Learning Fog-invariant Features for Foggy Scene Segmentation</div>
          <!-- Author -->
          <div class="author">
                  <em>Sohyun Lee</em>,¬†Taeyoung Son,¬†and Suha Kwak  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>),</em> 2022
          </div>
              
            <!---->
           
            
              
           <!-- Additional info -->
            <span style="color:red"><b>(Best Paper Finalist, Oral Presentation)</b></span><br>

           <!-- Additional info3 -->
            <span style="color:green">Invited paper talk at V4AS Workshop @ CVPR 2022</span><br>
    
           <!-- Additional info2 -->
            <span style="color:green">Qualcomm Innovation Fellowship Winner</span>
            
              
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2204.01587" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://github.com/sohyun-l/fifo" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="http://cvlab.postech.ac.kr/research/FIFO/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Robust visual recognition under adverse weather conditions is of great importance in real-world applications. In this context, we propose a new method for learning semantic segmentation models robust against fog. Its key idea is to consider the fog condition of an image as its style and close the gap between images with different fog conditions in neural style spaces of a segmentation model. In particular, since the neural style of an image is in general affected by other factors as well as fog, we introduce a fog-pass filter module that learns to extract a fog-relevant factor from the style. Optimizing the fog-pass filter and the segmentation model alternately gradually closes the style gap between different fog conditions and allows to learn fog-invariant features in consequence. Our method substantially outperforms previous work on three real foggy image datasets. Moreover, it improves performance on both foggy and clear weather images, while existing methods often degrade performance on clear scenes.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">            
      <div class="col-sm-3">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/style-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/style-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/style-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/style.png" data-zoomable>

  </picture>

</figure>

      </div>
            
        <!-- Entry bib key -->
        <div id="kang2022style" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Style Neophile: Constantly Seeking Novel Styles for Domain Generalization</div>
          <!-- Author -->
          <div class="author">Juwon Kang,¬†
                  <em>Sohyun Lee</em>,¬†Namyup Kim,¬†and Suha Kwak  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>),</em> 2022
          </div>
              
            <!---->
           
            
              
           <!-- Additional info -->
            

           <!-- Additional info3 -->
            
    
           <!-- Additional info2 -->
            <span style="color:green">Qualcomm Innovation Fellowship Winner</span>
            
              
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://cvlab.postech.ac.kr/research/StyleNeophile/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This paper studies domain generalization via domain-invariant representation learning. Existing methods in this direction suppose that a domain can be characterized by styles of its images, and train a network using style-augmented data so that the network is not biased to particular style distributions. However, these methods are restricted to a finite set of styles since they obtain styles for augmentation from a fixed set of external images or by interpolating those of training data. To address this limitation and maximize the benefit of style augmentation, we propose a new method that synthesizes novel styles constantly during training. Our method manages multiple queues to store styles that have been observed so far, and synthesizes novel styles whose distribution is distinct from the distribution of styles in the queues. The style synthesis process is formulated as a monotone submodular optimization, thus can be conducted efficiently by a greedy algorithm. Extensive experiments on four public benchmarks demonstrate that the proposed method is capable of achieving state-of-the-art domain generalization performance.</p>
          </div>
        </div>
      </div>
</li>
</ol>
          </div>

          
          <!-- Services -->          <div class="services">
            <h2>Professional Services</h2>
            <div class="table-responsive">
              <table class="table table-sm table-borderless"> 
                <tr>
                  <td>
                    <strong>Organizer</strong>
<ul>
  <li>Women in Computer Vision Workshop (WiCV) at ACCV 2024</li>
</ul>

<strong>Journal Reviewer</strong>
<ul>
  <li>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</li>
</ul>

<strong>Conference Reviewer</strong>
<ul>
  <li>ICLR, NeurIPS, ICCV, CVPR, ECCV, AAAI, WACV, ACCV</li>
</ul>

 
                  </td>
                </tr> 
              </table>
            </div> 
          </div>

          
          <!-- Services -->          <div class="honors">
            <h2>Honors and Awards</h2>
            <div class="table-responsive">
              <table class="table table-sm table-borderless"> 
                <tr>
                  <td>
                    <ul>
  <li>POSTECHIAN fellowship awards, POSTECH, 2023</li>
  <li>POSTECH Research Performance Contest (Excellence Award), POSTECH, 2023</li>
  <li>BK21 Best Paper Award (Grand Prize), POSTECH GSAI, 2023</li>
  <li>
<strong>Qualcomm Innovation Fellowship Winner (3 times)</strong>, Qualcomm Korea Corp., 2022
    <ul>
      <li>FIFO: Learning Fog-invariant Features for Foggy Scene Segmentation (CVPR 2022, Best Paper Finalist)</li>
      <li>Style Neophile: Constantly Seeking Novel Styles for Domain Generalization (CVPR 2022)</li>
      <li>Combating Label Distribution Shift for Active Domain Adaptation (ECCV 2022)</li>
    </ul>
  </li>
  <li>
<strong>CVPR Best Paper Finalist</strong>, CVPR, 2022
    <ul>
      <li>Awarded to Top 0.4% (33 of 8161 papers)</li>
      <li>FIFO: Learning Fog-invariant Features for Foggy Scene Segmentation</li>
    </ul>
  </li>
  <li>IPIU Best Paper Award (Gold Prize), IPIU, 2022</li>
  <li>POSTECH Creative Self-Research Scholarship, POSTECH GSAI, 2020</li>
</ul>
 
                  </td>
                </tr> 
              </table>
            </div> 
          </div>

              

        </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        ¬© Copyright 2025 Sohyun  Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>

