<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Sohyun  Lee | Publications</title>
    <meta name="author" content="Sohyun  Lee" />
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
" />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/smile-o"/>
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://sohyun-l.github.io/publications/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://sohyun-l.github.io/"><span class="font-weight-bold">Sohyun</span>   Lee</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              <!---->
              <!-- Blog -->
              <!--<li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li>-->
                
              <!-- CV -->
              <li class="nav-item">
                <a class="nav-link" href="/assets/Resume.pdf" target="_blank" rel="noopener noreferrer">Curriculum Vitae</a>
              </li>
                
              <!-- Other pages -->
              <li class="nav-item active">
                <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">Publications</h1>
            <p class="post-description"></p>
          </header>

          <article>
            <!-- _pages/publications.md -->
<div class="publications">
  <h2 class="year">2025</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">            
      <div class="col-sm-3">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/garasam_teaser-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/garasam_teaser-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/garasam_teaser-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/garasam_teaser.png" data-zoomable="">

  </picture>

</figure>

      </div>
            
        <!-- Entry bib key -->
        <div id="lee2025garasam" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">GaRA-SAM: Robustifying Segment Anything Model with Gated-Rank Adaptation</div>
          <!-- Author -->
          <div class="author">
                  <em>Sohyun Lee</em>, Yeho Gwon, Lukas Hoyer, and Suha Kwak  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Conference on Neural Information Processing Systems (<b>NeurIPS</b>),</em> 2025
          </div>
              
            <!---->
           
            
              
           <!-- Additional info -->
            

           <!-- Additional info3 -->
            
    
           <!-- Additional info2 -->
            <span style="color:green">ICCV 2025 Workshop on Building Foundation Models You Can Trust</span>
            
              
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2506.02882" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Improving robustness of the Segment Anything Model (SAM) to input degradations is critical for its deployment in high-stakes applications such as autonomous driving and robotics. Our approach to this challenge prioritizes three key aspects: first, parameter efficiency to maintain the inherent generalization capability of SAM; second, fine-grained and input-aware robustification to precisely address the input corruption; and third, adherence to standard training protocols for ease of training. To this end, we propose gated-rank adaptation (GaRA). GaRA introduces lightweight adapters into intermediate layers of the frozen SAM, where each adapter dynamically adjusts the effective rank of its weight matrix based on the input by selectively activating (rank-1) components of the matrix using a learned gating module. This adjustment enables fine-grained and input-aware robustification without compromising the generalization capability of SAM. Our model, GaRA-SAM, significantly outperforms prior work on all robust segmentation benchmarks. In particular, it surpasses the previous best IoU score by up to 21.3%p on ACDC, a challenging real corrupted image dataset.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">            
      <div class="col-sm-3">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/testdg_teaser-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/testdg_teaser-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/testdg_teaser-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/testdg_teaser.png" data-zoomable="">

  </picture>

</figure>

      </div>
            
        <!-- Entry bib key -->
        <div id="lee2025dicotta" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">TestDG: Test-time Domain Generalization for Continual Test-time Adaptation</div>
          <!-- Author -->
          <div class="author">
                  <em>Sohyun Lee</em>, Nayeong Kim, Juwon Kang, Seongjoon Oh, and Suha Kwak  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>arXiv preprint,</em> 2025
          </div>
              
            <!---->
           
            
              
           <!-- Additional info -->
            

           <!-- Additional info3 -->
            
    
           <!-- Additional info2 -->
            
            
              
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2504.04981" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://github.com/sohyun-l/TestDG" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This paper studies continual test-time adaptation (CTTA), the task of adapting a model to constantly changing unseen domains in testing while preserving previously learned knowledge. Existing CTTA methods mostly focus on adaptation to the current test domain only, overlooking generalization to arbitrary test domains a model may face in the future. To tackle this limitation, we present a novel online test-time domain generalization framework for CTTA, dubbed TestDG. TestDG aims to learn features invariant to both current and previous test domains on the fly during testing, improving the potential for effective generalization to future domains. To this end, we propose a new model architecture and a test-time adaptation strategy dedicated to learning domain-invariant features, along with a new data structure and optimization algorithm for effectively managing information from previous test domains. TestDG achieved state of the art on four public CTTA benchmarks. Moreover, it showed superior generalization to unseen test domains.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2024</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">            
      <div class="col-sm-3">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/frest-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/frest-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/frest-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/frest.png" data-zoomable="">

  </picture>

</figure>

      </div>
            
        <!-- Entry bib key -->
        <div id="lee2024frest" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">FREST: Feature RESToration for Semantic Segmentation under Multiple Adverse Conditions</div>
          <!-- Author -->
          <div class="author">
                  <em>Sohyun Lee</em>, Namyup Kim, Sungyeon Kim, and Suha Kwak  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>European Conference on Computer Vision (<b>ECCV</b>),</em> 2024
          </div>
              
            <!---->
           
            
              
           <!-- Additional info -->
            

           <!-- Additional info3 -->
            
    
           <!-- Additional info2 -->
            
            
              
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2407.13437" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://sohyun-l.github.io/frest/" class="btn btn-sm z-depth-0" role="button">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Robust semantic segmentation under adverse conditions is of great importance in real-world applications. To address this challenging task in practical scenarios where labeled normal condition images are not accessible in training, we propose FREST, a novel feature restoration framework for source-free domain adaptation (SFDA) of semantic segmentation to adverse conditions. FREST alternates two steps: (1) learning the condition embedding space that only separates the condition information from the features and (2) restoring features of adverse condition images on the learned condition embedding space. By alternating these two steps, FREST gradually restores features where the effect of adverse conditions is reduced. FREST achieved a state of the art on two public benchmarks (\ie, ACDC and RobotCar) for SFDA to adverse conditions. Moreover, it shows superior generalization ability on unseen datasets.</p>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2023</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">            
      <div class="col-sm-3">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/active_learning-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/active_learning-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/active_learning-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/active_learning.png" data-zoomable="">

  </picture>

</figure>

      </div>
            
        <!-- Entry bib key -->
        <div id="sehyun2023active" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Active Learning for Semantic Segmentation with Multi-class Label Query</div>
          <!-- Author -->
          <div class="author">Sehyun Hwang, 
                  <em>Sohyun Lee</em>, Hoyoung Kim, Minhyeon Oh, Jungseul Ok, and Suha Kwak  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Conference on Neural Information Processing Systems (<b>NeurIPS</b>),</em> 2023
          </div>
              
            <!---->
           
            
              
           <!-- Additional info -->
            

           <!-- Additional info3 -->
            
    
           <!-- Additional info2 -->
            
            
              
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2309.09319" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://github.com/sehyun03/MulActSeg" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This paper proposes a new active learning method for semantic segmentation. The core of our method lies in a new annotation query design. It samples informative local image regions (, superpixels), and for each of such regions, asks an oracle for a multi-hot vector indicating all classes existing in the region. This multi-class labeling strategy is substantially more efficient than existing ones like segmentation, polygon, and even dominant class labeling in terms of annotation time per click. However, it introduces the class ambiguity issue in training since it assigns partial labels (, a set of candidate classes) to individual pixels. We thus propose a new algorithm for learning semantic segmentation while disambiguating the partial labels in two stages. In the first stage, it trains a segmentation model directly with the partial labels through two new loss functions motivated by partial label learning and multiple instance learning. In the second stage, it disambiguates the partial labels by generating pixel-wise pseudo labels, which are used for supervised learning of the model. Equipped with a new acquisition function dedicated to the multi-class labeling, our method outperformed previous work on Cityscapes and PASCAL VOC 2012 while spending less annotation cost.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">            
      <div class="col-sm-3">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/pid-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/pid-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/pid-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/pid.png" data-zoomable="">

  </picture>

</figure>

      </div>
            
        <!-- Entry bib key -->
        <div id="lee2023pid" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Human Pose Estimation in Extremely Low-Light Conditions</div>
          <!-- Author -->
          <div class="author">
                  <em>Sohyun Lee*</em>, Jaesung Rim*, Boseung Jeong, Geonu Kim, ByungJu Woo, Haechan Lee, Sunghyun Cho, and Suha Kwak
             (*equal contribution)  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>),</em> 2023
          </div>
              
            <!---->
           
            
              
           <!-- Additional info -->
            

           <!-- Additional info3 -->
            
    
           <!-- Additional info2 -->
            
            
              
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2303.15410" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://github.com/sohyun-l/ExLPose" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="http://cg.postech.ac.kr/research/ExLPose/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We study human pose estimation in extremely low-light images. This task is challenging due to the difficulty of collecting real low-light images with accurate labels, and severely corrupted inputs that degrade prediction quality significantly. To address the first issue, we develop a dedicated camera system and build a new dataset of real low-light images with accurate pose labels. Thanks to our camera system, each low-light image in our dataset is coupled with an aligned well-lit image, which enables accurate pose labeling and is used as privileged information during training. We also propose a new model and a new training strategy that fully exploit the privileged information to learn representation insensitive to lighting conditions. Our method demonstrates outstanding performance on real extremely low-light images, and extensive analyses validate that both of our model and dataset contribute to the success.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">            
      <div class="col-sm-3">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/sehyun-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/sehyun-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/sehyun-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/sehyun.png" data-zoomable="">

  </picture>

</figure>

      </div>
            
        <!-- Entry bib key -->
        <div id="sehyun2022combating" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Combating Label Distribution Shift for Active Domain Adaptation</div>
          <!-- Author -->
          <div class="author">Sehyun Hwang, 
                  <em>Sohyun Lee</em>, Sungyeon Kim, Jungseul Ok, and Suha Kwak  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>European Conference on Computer Vision (<b>ECCV</b>),</em> 2022
          </div>
              
            <!---->
           
            
              
           <!-- Additional info -->
            

           <!-- Additional info3 -->
            
    
           <!-- Additional info2 -->
            <span style="color:green">Qualcomm Innovation Fellowship Winner</span>
            
              
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2208.06604" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://github.com/sehyun03/ADA-label-distribution-matching" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="http://cvlab.postech.ac.kr/research/LAMDA/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We consider the problem of active domain adaptation (ADA) to unlabeled target data, of which subset is actively selected and labeled given a budget constraint. Inspired by recent analysis on a critical issue from label distribution mismatch between source and target in domain adaptation, we devise a method that addresses the issue for the first time in ADA. At its heart lies a novel sampling strategy, which seeks target data that best approximate the entire target distribution as well as being representative, diverse, and uncertain. The sampled target data are then used not only for supervised learning but also for matching label distributions of source and target domains, leading to remarkable performance improvement. On four public benchmarks, our method substantially outperforms existing methods in every adaptation scenario.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">            
      <div class="col-sm-3">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/fifo-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/fifo-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/fifo-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/fifo.png" data-zoomable="">

  </picture>

</figure>

      </div>
            
        <!-- Entry bib key -->
        <div id="lee2022fifo" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">FIFO: Learning Fog-invariant Features for Foggy Scene Segmentation</div>
          <!-- Author -->
          <div class="author">
                  <em>Sohyun Lee</em>, Taeyoung Son, and Suha Kwak  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>),</em> 2022
          </div>
              
            <!---->
           
            
              
           <!-- Additional info -->
            <span style="color:red"><b>(Best Paper Finalist, Oral Presentation)</b></span><br>

           <!-- Additional info3 -->
            <span style="color:green">Invited paper talk at V4AS Workshop @ CVPR 2022</span><br>
    
           <!-- Additional info2 -->
            <span style="color:green">Qualcomm Innovation Fellowship Winner</span>
            
              
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2204.01587" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://github.com/sohyun-l/fifo" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="http://cvlab.postech.ac.kr/research/FIFO/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Robust visual recognition under adverse weather conditions is of great importance in real-world applications. In this context, we propose a new method for learning semantic segmentation models robust against fog. Its key idea is to consider the fog condition of an image as its style and close the gap between images with different fog conditions in neural style spaces of a segmentation model. In particular, since the neural style of an image is in general affected by other factors as well as fog, we introduce a fog-pass filter module that learns to extract a fog-relevant factor from the style. Optimizing the fog-pass filter and the segmentation model alternately gradually closes the style gap between different fog conditions and allows to learn fog-invariant features in consequence. Our method substantially outperforms previous work on three real foggy image datasets. Moreover, it improves performance on both foggy and clear weather images, while existing methods often degrade performance on clear scenes.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">            
      <div class="col-sm-3">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/style-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/style-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/style-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/style.png" data-zoomable="">

  </picture>

</figure>

      </div>
            
        <!-- Entry bib key -->
        <div id="kang2022style" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Style Neophile: Constantly Seeking Novel Styles for Domain Generalization</div>
          <!-- Author -->
          <div class="author">Juwon Kang, 
                  <em>Sohyun Lee</em>, Namyup Kim, and Suha Kwak  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>),</em> 2022
          </div>
              
            <!---->
           
            
              
           <!-- Additional info -->
            

           <!-- Additional info3 -->
            
    
           <!-- Additional info2 -->
            <span style="color:green">Qualcomm Innovation Fellowship Winner</span>
            
              
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://cvlab.postech.ac.kr/research/StyleNeophile/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This paper studies domain generalization via domain-invariant representation learning. Existing methods in this direction suppose that a domain can be characterized by styles of its images, and train a network using style-augmented data so that the network is not biased to particular style distributions. However, these methods are restricted to a finite set of styles since they obtain styles for augmentation from a fixed set of external images or by interpolating those of training data. To address this limitation and maximize the benefit of style augmentation, we propose a new method that synthesizes novel styles constantly during training. Our method manages multiple queues to store styles that have been observed so far, and synthesizes novel styles whose distribution is distinct from the distribution of styles in the queues. The style synthesis process is formulated as a monotone submodular optimization, thus can be conducted efficiently by a greedy algorithm. Extensive experiments on four public benchmarks demonstrate that the proposed method is capable of achieving state-of-the-art domain generalization performance.</p>
          </div>
        </div>
      </div>
</li>
</ol>


</div>

          </article>

        </div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Sohyun  Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>

